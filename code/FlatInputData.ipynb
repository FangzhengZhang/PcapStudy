{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file is used to flat the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from bokeh.io import output_file, output_notebook, show\n",
    "from bokeh.models import (\n",
    "  GMapPlot, GMapOptions, ColumnDataSource, Circle, LogColorMapper, BasicTicker, ColorBar,\n",
    "    DataRange1d, PanTool, WheelZoomTool, BoxSelectTool\n",
    ")\n",
    "from bokeh.models.mappers import ColorMapper, LinearColorMapper\n",
    "from bokeh.palettes import Viridis5\n",
    "import re\n",
    "import math\n",
    "import ast\n",
    "from pandas.io.json import json_normalize\n",
    "from flatten_json import flatten\n",
    "import ProjectLookUpTable \n",
    "import socket, struct\n",
    "from binascii import hexlify\n",
    "import datetime\n",
    "import macaddress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_json_file_with_file_name(pcap_Json_file_name):\n",
    "    json_data_file = open(pcap_Json_file_name, encoding='utf-8')\n",
    "    file_data = json.load(json_data_file)\n",
    "    json_data_file.close()\n",
    "    return file_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type_map_for_json(json_record) map string value to int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_map_for_json(json_record)\n",
    "\n",
    "\n",
    "def IPV6_to_int(ipv6_addr):\n",
    "    return int(hexlify(socket.inet_pton(socket.AF_INET6, ipv6_addr)), 16)\n",
    "\n",
    "def IPV4_to_int(ip):\n",
    "    packedIP = socket.inet_aton(ip)\n",
    "    return struct.unpack(\"!L\", packedIP)[0]\n",
    "\n",
    "def type_map_for_json(json_record):\n",
    "    if type(json_record) != dict:\n",
    "        return True\n",
    "    for json_key in json_record.keys():\n",
    "        if type_map_for_json(json_record[json_key]):\n",
    "            if json_key in ProjectLookUpTable.wire_shark_type_lookup_map:\n",
    "                json_value_type = ProjectLookUpTable.wire_shark_type_lookup_map[json_key]\n",
    "                json_str_value = json_record[json_key]\n",
    "                # change number string to number \n",
    "                if json_value_type in ProjectLookUpTable.wire_shark_number_type_set:\n",
    "                    if '0x' in json_str_value:\n",
    "                        json_record[json_key] = int(json_str_value, 0)\n",
    "                    elif '.' in json_str_value:\n",
    "                        json_record[json_key] = float(json_str_value)\n",
    "                    else:\n",
    "                        json_record[json_key] = int(json_str_value)\n",
    "                elif json_value_type in ProjectLookUpTable.wire_shark_ipaddr_type_set:\n",
    "                    if ':' in json_str_value:\n",
    "                        json_record[json_key] = IPV6_to_int(json_str_value)\n",
    "                    else:\n",
    "                        json_record[json_key] = IPV4_to_int(json_str_value)\n",
    "                elif json_value_type == 'Ethernet or other MAC address':\n",
    "                    json_record[json_key] = int(macaddress.MAC(json_str_value))\n",
    "                # elif json_value_type == 'Date and time':\n",
    "                #     json_str_value = json_str_value[0:json_str_value.index('.')+6]\n",
    "                #     json_record[json_key] = datetime.datetime.strptime(json_str_value, '%b %d, %Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wanted_packets(local_wanted_packets, local_json_record_list):\n",
    "    rv = []\n",
    "    idx = 0\n",
    "    for record in local_json_record_list:\n",
    "        if local_wanted_packets[idx] in record['_source']['layers']:\n",
    "            rv.append(record)\n",
    "            idx += 1\n",
    "            if idx >= len(local_wanted_packets):\n",
    "                return rv\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop use less fields by use drop_json_fields(json_record, field_name_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def json_field_drop(json_record, path_list):\n",
    "    if path_list[0] not in json_record:\n",
    "        return\n",
    "    if(len(path_list) == 1):\n",
    "        del json_record[path_list.pop(0)]\n",
    "    else:\n",
    "        json_record = json_record[path_list.pop(0)]\n",
    "        json_field_drop(json_record, path_list)\n",
    "\n",
    "def drop_json_fields(json_record, field_name_path_list):\n",
    "    for name_path_str in field_name_path_list:\n",
    "        json_field_drop(json_record, name_path_str.split('->'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_drop_field_value_mapping(local_json_records, local_json_drop_field_path_list):\n",
    "    for local_json_record in local_json_records:\n",
    "        local_json_record = local_json_record['_source']['layers']\n",
    "        drop_json_fields(local_json_record, local_json_drop_field_path_list)\n",
    "        type_map_for_json(local_json_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json_records(json_record_list):\n",
    "    rv = []\n",
    "    for record in json_record_list:\n",
    "        record = record['_source']['layers']\n",
    "        rv.append(flatten(record))\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only for IcedId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IcedId_processing(local_key_field, local_IcedID_cookie_check_ls, first_http_record):\n",
    "    if local_key_field in first_http_record:\n",
    "        cookie_str = first_http_record[local_key_field]\n",
    "        cookie_field_set = set()\n",
    "        for tocken in cookie_str.split(';'):\n",
    "            cookie_field_set.add(tocken.split('=')[0].strip())\n",
    "        counter = 0\n",
    "        for key in local_IcedID_cookie_check_ls:\n",
    "            if key in cookie_field_set:\n",
    "                counter+=1\n",
    "        if counter == len(local_IcedID_cookie_check_ls):\n",
    "            first_http_record['IcedIdCookie'] = 1\n",
    "        else:\n",
    "            first_http_record['IcedIdCookie'] = 0\n",
    "    else:\n",
    "        first_http_record['IcedIdCookie'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the string to integer map so the ML process can run faster \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_str_to_int(local_record_str_to_int_map, local_flatten_json_records_list):\n",
    "    for flatten_json_record in local_flatten_json_records_list:\n",
    "        for field_name in flatten_json_record:\n",
    "            if isinstance(flatten_json_record[field_name], str):\n",
    "                if flatten_json_record[field_name] not in local_record_str_to_int_map:\n",
    "                    local_record_str_to_int_map[flatten_json_record[field_name]] = len(local_record_str_to_int_map)+1\n",
    "                flatten_json_record[field_name] = local_record_str_to_int_map[flatten_json_record[field_name]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Cast the list of json records into one record and load into panda\n",
    "def make_list_json_record_into_one_record(local_json_records_list_list, local_num_record):\n",
    "    local_col_name_on_offset = {}\n",
    "\n",
    "    # init set\n",
    "    for off_set in range(local_num_record):\n",
    "        local_col_name_on_offset[off_set] = set()\n",
    "\n",
    "    # get all unique fiels of each records on each offset\n",
    "    for local_json_records_list in local_json_records_list_list:\n",
    "        for off_set in range(local_num_record):\n",
    "            if off_set < len(local_json_records_list):\n",
    "                for json_field in local_json_records_list[off_set]:\n",
    "                    local_col_name_on_offset[off_set].add(json_field)\n",
    "\n",
    "    # use the offset and unique fields name to build df\n",
    "    local_overall_field_list = []\n",
    "    for off_set in range(local_num_record):\n",
    "        for field_name in local_col_name_on_offset[off_set]:\n",
    "            local_overall_field_list.append(field_name+'_'+str(off_set))\n",
    "    local_df = pd.DataFrame(columns=local_overall_field_list)\n",
    "\n",
    "    # load data into the df\n",
    "    for local_json_records_list in local_json_records_list_list:\n",
    "        local_record_array = []\n",
    "        for off_set in range(local_num_record):\n",
    "            if off_set < len(local_json_records_list):\n",
    "                local_json_record = local_json_records_list[off_set]\n",
    "                for df_col_field in local_col_name_on_offset[off_set]:\n",
    "                    if df_col_field in local_json_record:\n",
    "                        local_record_array.append(local_json_record[df_col_field])\n",
    "                    else:\n",
    "                        local_record_array.append(0)\n",
    "        local_df.loc[len(local_df.index)] = local_record_array\n",
    "    return local_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important variables \n",
    "json_records_list_list = []\n",
    "file_name_lists = [\"../data/2021-02-25 - TA551 (SHATHAK) BACK TO PUSHING ICEDID (BOKBOT)/only/only.json\"]\n",
    "wanted_packets = ['http', 'tls', 'tls', 'tls', 'tls']\n",
    "json_drop_field_path_list = ['frame->frame.time', 'frame->frame.time_delta_displayed', \n",
    "    'frame->frame.coloring_rule.name', 'frame->frame.coloring_rule.string',\n",
    "    'ip->ip.addr', 'ip->ip.host', 'tcp->tcp.port','http->http.request.line',\n",
    "    'tcp->tcp.payload', \n",
    "    'ip->ip.src_host', 'ip->ip.dst_host', 'http->http.cookie_tree',  \n",
    "    'tls->tls.record->tls.handshake->tls.handshake.random',\n",
    "    'tls->tls.record->tls.handshake->tls.handshake.random_tree',\n",
    "    'tls->tls.record->tls.handshake.ciphersuites',\n",
    "]\n",
    "IcedID_cookie_check_ls = {'__gads', '_gat', '_ga', '_u', '__io', '_gid'}\n",
    "record_str_to_int_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for  pcap_json_file_name in file_name_lists: \n",
    "    # Read Json files\n",
    "    json_record_list = file_json_file_with_file_name(pcap_json_file_name)\n",
    "    # make sure the start\n",
    "    json_record_list = get_wanted_packets(wanted_packets, json_record_list)\n",
    "    # Drop not wanted fields \n",
    "    start_drop_field_value_mapping(json_record_list,json_drop_field_path_list)\n",
    "    # Flate Json into field value map\n",
    "    flatten_json_records_list = flatten_json_records(json_record_list)\n",
    "    \n",
    "    # For IcedId, take the cookie and make them into fields \n",
    "    IcedId_processing(\"http_http.cookie\", IcedID_cookie_check_ls, flatten_json_records_list[0])\n",
    "\n",
    "    # Map String into int\n",
    "    map_str_to_int(record_str_to_int_map, flatten_json_records_list)\n",
    "    # save current file to list\n",
    "    json_records_list_list.append(flatten_json_records_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the list record into one record \n",
    "df = make_list_json_record_into_one_record(json_records_list_list, len(wanted_packets))\n",
    "df.to_csv('IcedId_only_data_set.csv')\n",
    "# output flated data and str map to files\n",
    "with open('IcedId_record_str_to_int_map.json', 'w') as fp:\n",
    "    json.dump(record_str_to_int_map, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: map string to it matching int code, cookie spliter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free and testing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ASN.1 object identifier',\n",
       " 'Boolean',\n",
       " 'Byte sequence',\n",
       " 'Character string',\n",
       " 'Date and time',\n",
       " 'Ethernet or other MAC address',\n",
       " 'Floating point (double-precision)',\n",
       " 'Frame number',\n",
       " 'IPv4 address',\n",
       " 'IPv6 address',\n",
       " 'Label',\n",
       " 'Signed integer (1 byte)',\n",
       " 'Signed integer (2 bytes)',\n",
       " 'Signed integer (4 bytes)',\n",
       " 'Signed integer (8 bytes)',\n",
       " 'Time offset',\n",
       " 'Unsigned integer (1 byte)',\n",
       " 'Unsigned integer (2 bytes)',\n",
       " 'Unsigned integer (3 bytes)',\n",
       " 'Unsigned integer (4 bytes)',\n",
       " 'Unsigned integer (8 bytes)',\n",
       " 'Unsigned integer, 2 bytes',\n",
       " 'Unsigned integer, 4 bytes'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wire_shark_type = set()\n",
    "for key,val in ProjectLookUpTable.wire_shark_type_lookup_map.items():\n",
    "    wire_shark_type.add(val)\n",
    "wire_shark_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wire_shark_type_lookup_map = {}\n",
    "wire_shark_description_lookup_map = {}\n",
    "for i in range(len(field_list)):\n",
    "    wire_shark_type_lookup_map[field_list[i]] = type_list[i]\n",
    "    wire_shark_description_lookup_map[field_list[i]] = description_list[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e80b0b5753b5ad0cd2ffb86a3051736a97662009f3839a1ffdb9989820cc32e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
