{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file is used to flat the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from bokeh.io import output_file, output_notebook, show\n",
    "from bokeh.models import (\n",
    "  GMapPlot, GMapOptions, ColumnDataSource, Circle, LogColorMapper, BasicTicker, ColorBar,\n",
    "    DataRange1d, PanTool, WheelZoomTool, BoxSelectTool\n",
    ")\n",
    "from bokeh.models.mappers import ColorMapper, LinearColorMapper\n",
    "from bokeh.palettes import Viridis5\n",
    "import re\n",
    "import math\n",
    "import ast\n",
    "from pandas.io.json import json_normalize\n",
    "from flatten_json import flatten\n",
    "import ProjectLookUpTable \n",
    "import socket, struct\n",
    "from binascii import hexlify\n",
    "import datetime\n",
    "import macaddress\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_json_file_with_file_name(pcap_Json_file_name):\n",
    "    json_data_file = open(pcap_Json_file_name, encoding='utf-8')\n",
    "    file_data = json.load(json_data_file)\n",
    "    json_data_file.close()\n",
    "    return file_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type_map_for_json(json_record) map string value to int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_map_for_json(json_record)\n",
    "\n",
    "\n",
    "def IPV6_to_int(ipv6_addr):\n",
    "    return int(hexlify(socket.inet_pton(socket.AF_INET6, ipv6_addr)), 16)\n",
    "\n",
    "def IPV4_to_int(ip):\n",
    "    packedIP = socket.inet_aton(ip)\n",
    "    return struct.unpack(\"!L\", packedIP)[0]\n",
    "\n",
    "def type_map_for_json(json_record):\n",
    "    if type(json_record) != dict:\n",
    "        return True\n",
    "    for json_key in json_record.keys():\n",
    "        if type_map_for_json(json_record[json_key]):\n",
    "            if json_key in ProjectLookUpTable.wire_shark_type_lookup_map:\n",
    "                json_value_type = ProjectLookUpTable.wire_shark_type_lookup_map[json_key]\n",
    "                json_str_value = json_record[json_key]\n",
    "                # change number string to number \n",
    "                if isinstance(json_str_value, int) or isinstance(json_str_value, float):\n",
    "                    json_record[json_key] = json_value_type\n",
    "                elif json_value_type in ProjectLookUpTable.wire_shark_number_type_set:\n",
    "                    try:\n",
    "                        if '0x' in json_str_value:\n",
    "                            json_record[json_key] = int(json_str_value, 0)\n",
    "                        elif '.' in json_str_value:\n",
    "                            json_record[json_key] = float(json_str_value)\n",
    "                        else:\n",
    "                            json_record[json_key] = int(json_str_value)\n",
    "                    except:\n",
    "                        json_record[json_key] = 0\n",
    "                elif json_value_type in ProjectLookUpTable.wire_shark_ipaddr_type_set:\n",
    "                    try:\n",
    "                        if ':' in json_str_value:\n",
    "                            json_record[json_key] = IPV6_to_int(json_str_value)\n",
    "                        else:\n",
    "                            json_record[json_key] = IPV4_to_int(json_str_value)\n",
    "                    except:\n",
    "                        json_record[json_key] = json_str_value\n",
    "                elif json_value_type == 'Ethernet or other MAC address':\n",
    "                    try:\n",
    "                        json_record[json_key] = int(macaddress.MAC(json_str_value))\n",
    "                    except:\n",
    "                        json_record[json_key] = json_str_value\n",
    "                # elif json_value_type == 'Date and time':\n",
    "                #     json_str_value = json_str_value[0:json_str_value.index('.')+6]\n",
    "                #     json_record[json_key] = datetime.datetime.strptime(json_str_value, '%b %d, %Y %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wanted_packets(local_wanted_packets, local_json_record_list):\n",
    "    rv = []\n",
    "    idx = 0\n",
    "    for record in local_json_record_list:\n",
    "        if local_wanted_packets[idx] in record['_source']['layers']:\n",
    "            rv.append(record)\n",
    "            idx += 1\n",
    "            if idx >= len(local_wanted_packets):\n",
    "                return rv\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop use less fields by use drop_json_fields(json_record, field_name_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def json_field_drop(json_record, path_list):\n",
    "    if path_list[0] not in json_record:\n",
    "        return\n",
    "    if(len(path_list) == 1):\n",
    "        del json_record[path_list.pop(0)]\n",
    "    else:\n",
    "        json_record = json_record[path_list.pop(0)]\n",
    "        json_field_drop(json_record, path_list)\n",
    "\n",
    "def drop_json_fields(json_record, field_name_path_list):\n",
    "    for name_path_str in field_name_path_list:\n",
    "        json_field_drop(json_record, name_path_str.split('->'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_drop_field_value_mapping(local_json_records, local_json_drop_field_path_list):\n",
    "    for local_json_record in local_json_records:\n",
    "        local_json_record = local_json_record['_source']['layers']\n",
    "        drop_json_fields(local_json_record, local_json_drop_field_path_list)\n",
    "        type_map_for_json(local_json_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json_records(json_record_list):\n",
    "    rv = []\n",
    "    for record in json_record_list:\n",
    "        record = record['_source']['layers']\n",
    "        rv.append(flatten(record))\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only for IcedId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IcedId_processing(local_key_field, local_IcedID_cookie_check_ls, first_http_record):\n",
    "    if local_key_field in first_http_record:\n",
    "        cookie_str = first_http_record[local_key_field]\n",
    "        cookie_field_set = set()\n",
    "        for tocken in cookie_str.split(';'):\n",
    "            cookie_field_set.add(tocken.split('=')[0].strip())\n",
    "        counter = 0\n",
    "        for key in local_IcedID_cookie_check_ls:\n",
    "            if key in cookie_field_set:\n",
    "                counter+=1\n",
    "        if counter == len(local_IcedID_cookie_check_ls):\n",
    "            first_http_record['IcedIdCookie'] = 1\n",
    "        else:\n",
    "            first_http_record['IcedIdCookie'] = 0\n",
    "    else:\n",
    "        first_http_record['IcedIdCookie'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the string to integer map so the ML process can run faster \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_str_to_int(local_record_str_to_int_map, local_flatten_json_records_list):\n",
    "    for flatten_json_record in local_flatten_json_records_list:\n",
    "        for field_name in flatten_json_record:\n",
    "            if isinstance(flatten_json_record[field_name], str):\n",
    "                if flatten_json_record[field_name] not in local_record_str_to_int_map:\n",
    "                    local_record_str_to_int_map[flatten_json_record[field_name]] = len(local_record_str_to_int_map)+1\n",
    "                flatten_json_record[field_name] = local_record_str_to_int_map[flatten_json_record[field_name]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_header_of_each_col(local_num_record, malware_json_records_list_list, normal_json_records_list_list):\n",
    "    local_col_name_on_offset = {}\n",
    "\n",
    "    # init set\n",
    "    for off_set in range(local_num_record):\n",
    "        local_col_name_on_offset[off_set] = set()\n",
    "\n",
    "    # get all unique fiels of each records on each offset\n",
    "    for local_json_records_list in malware_json_records_list_list:\n",
    "        for off_set in range(local_num_record):\n",
    "            if off_set < len(local_json_records_list):\n",
    "                for json_field in local_json_records_list[off_set]:\n",
    "                    local_col_name_on_offset[off_set].add(json_field)\n",
    "\n",
    "    # get all unique fiels of each records on each offset\n",
    "    for local_json_records_list in normal_json_records_list_list:\n",
    "        for off_set in range(local_num_record):\n",
    "            if off_set < len(local_json_records_list):\n",
    "                for json_field in local_json_records_list[off_set]:\n",
    "                    local_col_name_on_offset[off_set].add(json_field)\n",
    "\n",
    "    return local_col_name_on_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Cast the list of json records into one record and load into panda\n",
    "def make_list_json_record_into_one_record(local_col_name_on_offset, local_json_records_list_list, local_num_record):\n",
    "\n",
    "    # use the offset and unique fields name to build df\n",
    "    local_overall_field_list = []\n",
    "    for off_set in range(local_num_record):\n",
    "        for field_name in local_col_name_on_offset[off_set]:\n",
    "            local_overall_field_list.append(field_name+'_'+str(off_set))\n",
    "    local_df = pd.DataFrame(columns=local_overall_field_list)\n",
    "    try:\n",
    "        # load data into the df\n",
    "        for local_json_records_list in local_json_records_list_list:\n",
    "            local_record_array = []\n",
    "            for off_set in range(local_num_record):\n",
    "                if off_set < len(local_json_records_list):\n",
    "                    local_json_record = local_json_records_list[off_set]\n",
    "                    for df_col_field in local_col_name_on_offset[off_set]:\n",
    "                        if df_col_field in local_json_record:\n",
    "                            local_record_array.append(local_json_record[df_col_field])\n",
    "                        else:\n",
    "                            local_record_array.append(0)\n",
    "            local_df.loc[len(local_df.index)] = local_record_array\n",
    "    except:\n",
    "        print(local_record_array)\n",
    "    return local_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important variables \n",
    "json_records_list_list = []\n",
    "file_name_lists = []\n",
    "\n",
    "for top_file_path in os.listdir(\"../../data/IcedId\"):\n",
    "    local_path = '../../data/IcedId/'+top_file_path + '/only/only.json'\n",
    "    if os.path.exists(local_path):\n",
    "        file_name_lists.append(local_path)\n",
    "\n",
    "wanted_packets = ['http', 'tls', 'tls', 'tls', 'tls']\n",
    "json_drop_field_path_list = ['frame->frame.time', 'frame->frame.time_delta_displayed', \n",
    "    'frame->frame.coloring_rule.name', 'frame->frame.coloring_rule.string',\n",
    "    'ip->ip.addr', 'ip->ip.host', 'tcp->tcp.port','http->http.request.line',\n",
    "    'tcp->tcp.payload', \n",
    "    'ip->ip.src_host', 'ip->ip.dst_host', 'http->http.cookie_tree',  \n",
    "    'tls->tls.record->tls.handshake->tls.handshake.random',\n",
    "    'tls->tls.record->tls.handshake->tls.handshake.random_tree',\n",
    "    'tls->tls.record->tls.handshake.ciphersuites',\n",
    "]\n",
    "IcedID_cookie_check_ls = {'__gads', '_gat', '_ga', '_u', '__io', '_gid'}\n",
    "record_str_to_int_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/IcedId/2021-02-25 - TA551 (SHATHAK) BACK TO PUSHING ICEDID (BOKBOT)/only/only.json',\n",
       " '../../data/IcedId/2021-04-29 (THURSDAY) - TA551 (SHATHAK) PUSHES ICEDID (BOKBOT)/only/only.json',\n",
       " '../../data/IcedId/2021-05-24 (MONDAY) - TA551 (SHATHAK) WORD DOCS PUSH ICEDID (BOKBOT)/only/only.json',\n",
       " '../../data/IcedId/2021-06-02 (WEDNESDAY) - TA551 (SHATHAK) WORD DOCS PUSH ICEDID (BOKBOT)/only/only.json',\n",
       " '../../data/IcedId/2021-12-10 (FRIDAY) - TA551 (SHATHAK) ICEDID (BOKBOT) WITH COBALT STRIKE AND DARK VNC/only/only.json',\n",
       " '../../data/IcedId/2022-01-05 (WEDNESDAY) - TA551 (SHATHAK) PUSHES ICEDID (BOKBOT) WITH COBALT STRIKE/only/only.json',\n",
       " '../../data/IcedId/2022-01-06 (THURSDAY) - TA551 (SHATHAK) PUSHES ICEDID (BOKBOT)/only/only.json',\n",
       " '../../data/IcedId/2022-01-12 (WEDNESDAY) - ICEDID (BOKBOT) WITH COBALT STRIKE AND DARKVNC/only/only.json',\n",
       " '../../data/IcedId/2022-05-10 (TUESDAY) - TA578 CONTACT FORMS CAMPAIGN -- ICEDID (BOKBOT) -- COBALT STRIKE/only/only.json',\n",
       " '../../data/IcedId/2022-05-23 (MONDAY) ICEDID (BOKBOT) INFECTION WITH DARKVNC TRAFFIC/only/only.json',\n",
       " '../../data/IcedId/2022-07-06 (WEDNESDAY) - TA578 STOLEN IMAGES EVIDENCE -- ICEDID (BOKBOT) -- DARK VNC & COBALT STRIKE/only/only.json',\n",
       " '../../data/IcedId/2022-07-21 (THURSDAY) - ICEDID (BOKBOT) INFECTION WITH DARK VNC AND COBALT STRIKE/only/only.json',\n",
       " '../../data/IcedId/2022-07-25 (MONDAY) - ICEDID (BOKBOT) INFECTION WITH COBALT STRIKE/only/only.json']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for  pcap_json_file_name in file_name_lists: \n",
    "    # Read Json files\n",
    "    json_record_list = file_json_file_with_file_name(pcap_json_file_name)\n",
    "    # make sure the start\n",
    "    json_record_list = get_wanted_packets(wanted_packets, json_record_list)\n",
    "    # Drop not wanted fields \n",
    "    start_drop_field_value_mapping(json_record_list,json_drop_field_path_list)\n",
    "    # Flate Json into field value map\n",
    "    flatten_json_records_list = flatten_json_records(json_record_list)\n",
    "    \n",
    "    # For IcedId, take the cookie and make them into fields \n",
    "    IcedId_processing(\"http_http.cookie\", IcedID_cookie_check_ls, flatten_json_records_list[0])\n",
    "\n",
    "    # Map String into int\n",
    "    map_str_to_int(record_str_to_int_map, flatten_json_records_list)\n",
    "    # save current file to list\n",
    "    json_records_list_list.append(flatten_json_records_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and output the normal traffic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_wanted_packets( local_wanted_packets, local_json_list):\n",
    "    local_valid_record_List_group = []\n",
    "    for local_json_list_idx in range(len(local_json_list)):\n",
    "        if local_json_list_idx+len(local_wanted_packets)-1 >= len(local_json_list):\n",
    "            break\n",
    "        json_record = local_json_list[local_json_list_idx]\n",
    "        if local_wanted_packets[0] in json_record['_source']['layers'] and local_wanted_packets[0] not in local_json_list[local_json_list_idx+1]['_source']['layers']:\n",
    "            local_valid_record_List_group.append(get_wanted_packets(wanted_packets, local_json_list[local_json_list_idx:]))\n",
    "    return local_valid_record_List_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_json_records_list_list = []\n",
    "\n",
    "normal_file_name_lists = []\n",
    "for top_file_path in os.listdir(\"../../data/IcedId_normal\"):\n",
    "    if top_file_path.endswith(\".json\"):\n",
    "        local_path = '../../data/IcedId_normal/'+top_file_path\n",
    "        normal_file_name_lists.append(local_path)\n",
    "\n",
    "\n",
    "jumper =[] \n",
    "\n",
    "for  pcap_json_file_name in normal_file_name_lists: \n",
    "    # Read Json files\n",
    "    json_record_list = file_json_file_with_file_name(pcap_json_file_name)\n",
    "\n",
    "    # make sure the start\n",
    "    json_record_list_group = get_normal_wanted_packets(wanted_packets, json_record_list)\n",
    "\n",
    "    for json_list in json_record_list_group:\n",
    "        jumper=json_list\n",
    "        # Drop not wanted fields \n",
    "        start_drop_field_value_mapping(json_list,json_drop_field_path_list)\n",
    "        # Flate Json into field value map\n",
    "        flatten_json_records_list = flatten_json_records(json_list)\n",
    "        # For IcedId, take the cookie and make them into fields \n",
    "        IcedId_processing(\"http_http.cookie\", IcedID_cookie_check_ls, flatten_json_records_list[0])\n",
    "        # Map String into int\n",
    "        map_str_to_int(record_str_to_int_map, flatten_json_records_list)\n",
    "        # save current file to list\n",
    "        normal_json_records_list_list.append(flatten_json_records_list)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output data table to CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_on_offset = get_df_header_of_each_col(len(wanted_packets), json_records_list_list, []) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the IceId record into one record \n",
    "IcedId_df = make_list_json_record_into_one_record(col_name_on_offset, json_records_list_list, len(wanted_packets))\n",
    "IcedId_df.to_csv('IcedId_only_data_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 2398856037, 2048, 0, 122, 0, 2, 0, 112, 0, 2, 0, 0, 1666988174.225725, 80, 1, 3900358604, 198112232540353, 127874741723750, 121, 152, 0, 0, 11808409, 120, 7621928, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 122, 1, 24, 0, 0, 1021, 3232237710, 128, 1, 0, 0, 0, 7621928, 0, 0, 123, 6, 20, 128, 129, 0, 0, 0, 0, 1021, 196.224102, 0, 0, 123, 127874741723750, 180.068648, 0, 0, 0, 0, 60.01891, 0, 1, 0, 51804, 0, 126, 0, 8, 0.571264, 112, 20, 0, 120, 448, 0, 0, 198112232540353, 382, 0, 0, 7, 0, 0, 1, 166, 19379, 0, 337, 93, 1, 0, 0, 0, -1, 0, 64545, 0, 4, 0, 166, 0.0, 449, 0, 0, 1, 0, 0, 127, 0, 0, 11808409, 0, 0, 0, 0, 0, 121, 1114945811, 6, 0, 0, 0, 0, 0, 169, 234, 0, 174, 0, 172, 169, 0, 172, 172, 0, 122, 0, 169, 173, 169, 168, 0, 0, 173, 173, 0, 167, 172, 0, 0, 172, 0, 0, 168, 168, 0, 172, 0, 170, 0, 170, 172, 0, 121, 173, 172, 173, 0, 171, 120, 233, 172, 171, 0, 169, 169, 173, 173, 0, 169, 0, 255, 0, 172, 173, 0, 0, 172, 122, 169, 172, 0, 0, 0, 172, 169, 0, 172, 174, 173, 172, 169, 0, 171, 169, 172, 123, 173, 0, 0, 0, 0, 0, 0, 173, 172, 0, 0, 173, 172, 0, 274, 168, 167, 173, 169, 169, 123, 0, 170, 0, 167, 169, 169, 0, 173, 0, 0, 167, 172, 0, 0, 0, 169, 172, 0, 172, 172, 173, 167, 172, 0, 0, 0, 168, 173, 120, 0, 0, 172, 0, 168, 0, 0, 173, 0, 0, 169, 0, 170, 0, 0, 168, 169, 172, 0, 173, 0, 0, 0, 0, 172, 0, 0, 169, 168, 172, 173, 168, 168, 0, 166, 169, 172, 0, 0, 175, 0, 0, 172, 0, 172, 171, 0, 172, 169, 173, 0, 168, 167, 168, 172, 0, 18, 0, 0, 169, 172, 173, 0, 172, 171, 0, 173, 0, 169, 121, 168, 6, 0, 0, 0, 173]\n"
     ]
    }
   ],
   "source": [
    "# cast the normal record into one record \n",
    "normal_df = make_list_json_record_into_one_record(col_name_on_offset, normal_json_records_list_list, len(wanted_packets))\n",
    "normal_df.to_csv('normal_IcedId_only_data_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output flated data and str map to files\n",
    "with open('IcedId_record_str_to_int_map.json', 'w') as fp:\n",
    "    json.dump(record_str_to_int_map, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free and testing space, do not run the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wire_shark_type = set()\n",
    "for key,val in ProjectLookUpTable.wire_shark_type_lookup_map.items():\n",
    "    wire_shark_type.add(val)\n",
    "wire_shark_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wire_shark_type_lookup_map = {}\n",
    "wire_shark_description_lookup_map = {}\n",
    "for i in range(len(field_list)):\n",
    "    wire_shark_type_lookup_map[field_list[i]] = type_list[i]\n",
    "    wire_shark_description_lookup_map[field_list[i]] = description_list[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e80b0b5753b5ad0cd2ffb86a3051736a97662009f3839a1ffdb9989820cc32e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
